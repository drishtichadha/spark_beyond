{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88784c53-7ce1-40f9-acea-a5dc9e713ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Spark Tune - Enhanced Features Demo\n",
    "\n",
    "This notebook demonstrates all the new features added to Spark Tune:\n",
    "\n",
    "1. **Data Quality Checking** - PySpark-native quality analysis\n",
    "2. **YData Profiling** - Comprehensive data profiling\n",
    "3. **Time Series Detection** - Automatic temporal structure detection\n",
    "4. **Enhanced Preprocessing** - Missing values, outliers, scaling, rare categories\n",
    "5. **Baseline Models** - Simple model comparison\n",
    "6. **LightAutoML** - Automated machine learning\n",
    "7. **Model Comparison** - Impact analysis framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfec2230-3d86-4edb-9358-87542fe216be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62604ac0-9d9e-4121-a30b-2598c3a7862f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -r /Workspace/Users/yadvendra@aidetic.in/spark_beyond/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d45f46-29b2-4600-a8c8-79f9c9aef874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialize MLflow Tracking\n",
    "\n",
    "MLflow tracks all model training runs (baselines, XGBoost, AutoML) so you can compare experiments later.\n",
    "Uses local file-based tracking (`./mlruns/` directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.services.mlflow_service import MLflowTracker\n",
    "\n",
    "# Initialize MLflow tracker (uses local file-based tracking in ./mlruns/)\n",
    "tracker = MLflowTracker(session_id=\"hdfc_demo\")\n",
    "\n",
    "if tracker.is_enabled:\n",
    "    exp_info = tracker.get_experiment_info()\n",
    "    print(f\"MLflow tracking enabled\")\n",
    "    print(f\"  Experiment: {exp_info['name']}\")\n",
    "    print(f\"  Experiment ID: {exp_info['experiment_id']}\")\n",
    "    print(f\"  Artifact location: {exp_info['artifact_location']}\")\n",
    "else:\n",
    "    print(\"MLflow tracking is disabled (mlflow not installed or init failed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5511793c-b1b8-45fe-b6c5-65c34643ef9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Initialize Spark and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f7bfb1-757c-43f5-9239-25bf14adb367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialize Spark\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .appName(\"Spark Tune Enhanced Demo\") \\\n",
    "#     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62196373-9e6e-4c91-9e8c-37c380e6c895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.utils import process_col_names\n",
    "\n",
    "# Load the bank marketing dataset\n",
    "# df = spark.read.options(\n",
    "#     header=True,\n",
    "#     inferSchema='True',\n",
    "#     delimiter=','\n",
    "# ).csv(\"dbfs:/Workspace/Users/yadvendra@aidetic.in/spark_beyond/backend/data/bank-additional-full.csv\")\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"/Volumes/aidetic_databricks/default/credit_card_transactions/credit_card_transactions.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df = df.drop(\"Unnamed: 0\")\n",
    "\n",
    "# Process column names\n",
    "df = process_col_names(df)\n",
    "\n",
    "print(f\"Dataset shape: {df.count():,} rows x {len(df.columns)} columns\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d44cef1c-8b91-451c-b04a-610cedd1bae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 2. üìã Data Quality Checking (NEW)\n",
    "\n",
    "The `DataQualityChecker` performs PySpark-native quality analysis without converting to Pandas,\n",
    "making it suitable for very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fda6c78-90e9-4e49-8f81-8df62fcfbed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from backend.core.profiling.data_quality import DataQualityChecker\n",
    "\n",
    "# # Run data quality checks\n",
    "# quality_checker = DataQualityChecker(df)\n",
    "# quality_report = quality_checker.run_all_checks()\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# print(\"DATA QUALITY REPORT\")\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"\\nüìä Quality Score: {quality_report.quality_score}/100\")\n",
    "# print(f\"üìà Row Count: {quality_report.row_count:,}\")\n",
    "# print(f\"üìã Column Count: {quality_report.column_count}\")\n",
    "# print(f\"üîÑ Duplicate Rows: {quality_report.duplicate_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904e8d8a-4eb8-4484-9389-05ad11be03ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Display quality issues\n",
    "# print(\"\\n‚ö†Ô∏è DATA QUALITY ISSUES:\")\n",
    "# print(\"-\" * 40)\n",
    "# if quality_report.issues:\n",
    "#     for issue in quality_report.issues[:10]:\n",
    "#         severity_icon = \"üî¥\" if issue['severity'] == 'high' else \"üü°\" if issue['severity'] == 'medium' else \"üü¢\"\n",
    "#         print(f\"{severity_icon} [{issue['severity'].upper()}] {issue['column']}: {issue['issue']}\")\n",
    "# else:\n",
    "#     print(\"‚úÖ No major quality issues detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c73ab7-31b2-4de4-a9a1-372be0146d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Display recommendations\n",
    "# print(\"\\nüí° PREPROCESSING RECOMMENDATIONS:\")\n",
    "# print(\"-\" * 40)\n",
    "# if quality_report.recommendations:\n",
    "#     for rec in quality_report.recommendations[:10]:\n",
    "#         priority_icon = \"üî¥\" if rec['priority'] == 'high' else \"üü°\" if rec['priority'] == 'medium' else \"üü¢\"\n",
    "#         print(f\"{priority_icon} [{rec['priority'].upper()}] {rec['column']}: {rec['action']}\")\n",
    "# else:\n",
    "#     print(\"‚úÖ No preprocessing recommendations needed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7791704f-8f40-4e10-8dbe-9231ff7ba2ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Detect outliers\n",
    "# print(\"\\nüìä OUTLIER DETECTION (IQR Method):\")\n",
    "# print(\"-\" * 40)\n",
    "# outliers = quality_checker.detect_outliers(method='iqr', threshold=1.5)\n",
    "\n",
    "# for col, stats in outliers.items():\n",
    "#     if stats['outlier_pct'] > 0:\n",
    "#         print(f\"  {col}: {stats['outlier_count']:,} outliers ({stats['outlier_pct']:.2f}%)\")\n",
    "#         print(f\"    Bounds: [{stats['lower_bound']:.2f}, {stats['upper_bound']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d8ef5a4-80c1-486c-a9d4-9712e9046b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 3. üìä YData Profiling (NEW)\n",
    "\n",
    "The `DataProfiler` generates comprehensive data profiles using ydata-profiling,\n",
    "with automatic sampling for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da14117d-aa5e-418d-9235-4af8cd9fa1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "from backend.core.utils.spark_pandas_bridge import spark_to_pandas_safe\n",
    "\n",
    "# df_pd\n",
    "\n",
    "profile = ProfileReport(df.toPandas(), title=\"Profiling Report\")\n",
    "\n",
    "profile.to_file(\"data_profiling_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639b46d3-ffd8-423f-89d4-39fe09df1bd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.profiling.ydata_profiler import DataProfiler, quick_profile\n",
    "\n",
    "# Quick profile (faster, minimal report)\n",
    "print(\"Generating quick profile...\")\n",
    "quick_stats = quick_profile(df, max_rows=10000)\n",
    "\n",
    "print(\"\\nüìà QUICK PROFILE SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "summary = quick_stats['summary']\n",
    "print(f\"  Rows: {summary.get('n_rows', 0):,}\")\n",
    "print(f\"  Columns: {summary.get('n_columns', 0)}\")\n",
    "print(f\"  Missing Cells: {summary.get('missing_cells_pct', 0):.2f}%\")\n",
    "print(f\"  Duplicate Rows: {summary.get('duplicate_rows_pct', 0):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7cd7b6-d894-4eab-8001-3067ed923c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display alerts\n",
    "print(\"\\n‚ö†Ô∏è DATA ALERTS:\")\n",
    "print(\"-\" * 40)\n",
    "if quick_stats['alerts']:\n",
    "    for alert in quick_stats['alerts'][:10]:\n",
    "        print(f\"  - {alert['column']}: {alert['type']}\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No alerts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3809f3-5e65-4044-bd33-be1354de561c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display profiling recommendations\n",
    "print(\"\\nüí° PROFILING RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "if quick_stats['recommendations']:\n",
    "    for rec in quick_stats['recommendations'][:10]:\n",
    "        priority_icon = \"üî¥\" if rec['priority'] == 'high' else \"üü°\" if rec['priority'] == 'medium' else \"üü¢\"\n",
    "        print(f\"{priority_icon} {rec['column']}: {rec['action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e3b35f-d928-4155-b42a-eeb9014e1b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 4. üéØ Problem Definition & Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48216ff-605a-499b-b524-1795f0be1b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.discovery import Problem, SchemaChecks\n",
    "\n",
    "# Define the ML problem\n",
    "problem = Problem(\n",
    "    target=\"is_fraud\",\n",
    "    type=\"classification\",\n",
    "    desired_result=1,\n",
    "    date_column=\"trans_date_trans_time\"\n",
    ")\n",
    "\n",
    "print(f\"Problem Type: {problem.type}\")\n",
    "print(f\"Target Column: {problem.target}\")\n",
    "print(f\"Desired Result: {problem.desired_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6973bd03-953b-459e-86d5-14cb555bf539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate schema\n",
    "schema_checker = SchemaChecks(dataframe=df, problem=problem)\n",
    "schema_info = schema_checker.check()\n",
    "\n",
    "print(f\"\\nüìã SCHEMA SUMMARY:\")\n",
    "print(f\"  Categorical columns: {len(schema_info['categorical'])}\")\n",
    "print(f\"  Numerical columns: {len(schema_info['numerical'])}\")\n",
    "print(f\"  Boolean columns: {len(schema_info['boolean'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00f5caf-cea5-422a-b3b3-656177c87e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 5. ‚è±Ô∏è Time Series Detection (NEW)\n",
    "\n",
    "The `detect_time_series_structure` function automatically identifies temporal patterns\n",
    "and recommends appropriate time-series features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6033600-75f5-4407-aeef-9166a8cd5c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.utils.time_series_detector import detect_time_series_structure, TimeSeriesFrequency\n",
    "\n",
    "# Detect time-series structure\n",
    "ts_info = detect_time_series_structure(df, schema_checker)\n",
    "\n",
    "print(\"\\n‚è±Ô∏è TIME SERIES DETECTION RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Is Time Series: {ts_info.is_time_series}\")\n",
    "print(f\"  Time Column: {ts_info.time_column or 'N/A'}\")\n",
    "print(f\"  Frequency: {ts_info.frequency.value if ts_info.frequency else 'N/A'}\")\n",
    "print(f\"  Entity Columns: {ts_info.entity_columns or 'N/A'}\")\n",
    "\n",
    "if ts_info.warnings:\n",
    "    print(\"\\n‚ö†Ô∏è Warnings:\")\n",
    "    for warning in ts_info.warnings:\n",
    "        print(f\"    - {warning}\")\n",
    "\n",
    "if ts_info.recommended_features:\n",
    "    print(\"\\nüí° Recommended Time-Series Features:\")\n",
    "    for feature in ts_info.recommended_features:\n",
    "        print(f\"    - {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eba03cd7-d723-4523-a17e-1dbb8565f871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 6. üîß Enhanced Preprocessing (NEW)\n",
    "\n",
    "The `EnhancedPreprocessor` provides feature-engine inspired transformations:\n",
    "- Missing value imputation (mean, median, mode, constant)\n",
    "- Outlier handling (IQR cap/remove, Z-score cap/remove)\n",
    "- Feature scaling (standard, minmax, robust)\n",
    "- Rare category grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5ae21a-04b7-4213-8cfa-5a622bf0b345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.features.preprocessing_enhanced import (\n",
    "    EnhancedPreprocessor, PreprocessingConfig,\n",
    "    ImputationStrategy, OutlierStrategy, ScalingStrategy\n",
    ")\n",
    "\n",
    "# Configure preprocessing\n",
    "config = PreprocessingConfig(\n",
    "    imputation_strategy=ImputationStrategy.MEDIAN,\n",
    "    outlier_strategy=OutlierStrategy.IQR_CAP,\n",
    "    outlier_threshold=1.5,\n",
    "    scaling_strategy=ScalingStrategy.ROBUST,\n",
    "    rare_category_threshold=0.01,  # 1%\n",
    "    rare_category_replacement=\"RARE\"\n",
    ")\n",
    "\n",
    "print(\"üìã PREPROCESSING CONFIGURATION:\")\n",
    "print(f\"  Imputation: {config.imputation_strategy.value}\")\n",
    "print(f\"  Outlier Handling: {config.outlier_strategy.value if config.outlier_strategy else 'None'}\")\n",
    "print(f\"  Scaling: {config.scaling_strategy.value}\")\n",
    "print(f\"  Rare Category Threshold: {config.rare_category_threshold*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb920c7-3b67-44e4-9801-44e20d74ff72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get column lists\n",
    "numerical_cols = schema_checker.get_typed_col(\"numerical\")\n",
    "categorical_cols = schema_checker.get_typed_col(\"categorical\")\n",
    "\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols[:5]}...\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcac1c83-79ff-42c4-9acd-ab40ac51ca7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply enhanced preprocessing\n",
    "preprocessor = EnhancedPreprocessor(df, config)\n",
    "\n",
    "# 1. Impute missing values\n",
    "print(\"\\n1Ô∏è‚É£ Imputing missing values...\")\n",
    "df_imputed = preprocessor.impute_missing_values(numerical_cols, strategy=ImputationStrategy.MEDIAN)\n",
    "print(f\"   ‚úÖ Imputed {len(numerical_cols)} numerical columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de888cd9-d639-4583-8396-1a44b587c9e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Handle outliers\n",
    "print(\"\\n2Ô∏è‚É£ Handling outliers (IQR capping)...\")\n",
    "preprocessor_outliers = EnhancedPreprocessor(df_imputed, config)\n",
    "df_no_outliers = preprocessor_outliers.handle_outliers(numerical_cols)\n",
    "\n",
    "# Show outlier handling stats\n",
    "outlier_params = preprocessor_outliers.get_fitted_params().get('outliers', {})\n",
    "print(f\"   ‚úÖ Capped outliers in {len(outlier_params)} columns\")\n",
    "for col, params in list(outlier_params.items())[:3]:\n",
    "    print(f\"      {col}: bounds [{params['lower_bound']:.2f}, {params['upper_bound']:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d579ebfd-01ff-48a9-a027-5bc658a2a3a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Group rare categories\n",
    "# print(\"\\n3Ô∏è‚É£ Grouping rare categories...\")\n",
    "# preprocessor_rare = EnhancedPreprocessor(df_no_outliers, config)\n",
    "# df_grouped = preprocessor_rare.group_rare_categories(categorical_cols)\n",
    "\n",
    "# # Show rare category grouping stats\n",
    "# rare_params = preprocessor_rare.get_fitted_params().get('rare_categories', {})\n",
    "# for col, params in rare_params.items():\n",
    "#     if params['count'] > 0:\n",
    "#         print(f\"   {col}: grouped {params['count']} rare categories into '{params['replacement']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5616ce73-0b07-48b8-9f17-59769ee22187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Scale features\n",
    "print(\"\\n4Ô∏è‚É£ Scaling features (Robust scaling)...\")\n",
    "# preprocessor_scale = EnhancedPreprocessor(df_grouped, config)\n",
    "preprocessor_scale = EnhancedPreprocessor(df_no_outliers, config)\n",
    "df_scaled = preprocessor_scale.scale_features(numerical_cols[:3])  # Just first 3 for demo\n",
    "\n",
    "scaling_params = preprocessor_scale.get_fitted_params().get('scaling', {})\n",
    "print(f\"   ‚úÖ Scaled {len(scaling_params)} columns\")\n",
    "for col, params in scaling_params.items():\n",
    "    print(f\"      {col}: median={params['median']:.2f}, IQR={params['iqr']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa8fce4-a59b-4825-a91e-4b18cc6453c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Cyclical encoding (bonus feature)\n",
    "print(\"\\n5Ô∏è‚É£ Cyclical encoding example...\")\n",
    "\n",
    "# Create a sample hour column for demo\n",
    "from pyspark.sql import functions as F\n",
    "df_with_hour = df_scaled.withColumn(\"hour\", F.lit(12))  # Mock hour column\n",
    "\n",
    "preprocessor_cyclical = EnhancedPreprocessor(df_with_hour, config)\n",
    "df_cyclical = preprocessor_cyclical.encode_cyclical_features(\"hour\", period=24)\n",
    "\n",
    "print(\"   Created: hour_sin, hour_cos columns\")\n",
    "df_cyclical.select(\"hour\", \"hour_sin\", \"hour_cos\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f825a17-a8dd-46c9-9442-d78184c39151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_with_hour.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "655d23cc-4e25-4264-8721-bf577556f238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ba43f5-9966-49af-9cae-cfb58de019b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from tsfresh.examples import load_robot_execution_failures\n",
    "# from tsfresh import extract_relevant_featuresa\n",
    "# df, y = load_robot_execution_failures()\n",
    "# X = extract_relevant_features(df, y, column_id='id', column_sort='time')\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6537a013-9165-49c8-ac72-f441c6936b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # from tsfresh import extract_features, extract_relevant_features, select_features\n",
    "# # from tsfresh.utilities.dataframe_functions import impute\n",
    "# # from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "\n",
    "# from tsfresh.convenience.bindings import extract_features_with_spark\n",
    "\n",
    "\n",
    "# extraction_settings = ComprehensiveFCParameters()\n",
    "\n",
    "# X = extract_features(df_with_hour, column_id='trans_num', column_sort='trans_date_trans_time',\n",
    "#                      default_fc_parameters=extraction_settings,\n",
    "#                      # we impute = remove all NaN features automatically\n",
    "#                      impute_function=impute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85664fe5-e932-4891-9c5b-4e2ccf02d91d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "757423c9-18e4-430d-9e6a-56180972b691",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing with Schema Check Fix"
    }
   },
   "outputs": [],
   "source": [
    "# from backend.core.features.process import PreProcessVariables\n",
    "\n",
    "# # Disable ANSI mode to prevent integer overflow on large feature values\n",
    "# # spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")\n",
    "\n",
    "# # # Create a new schema checker for the feature-engineered dataframe\n",
    "# # schema_checker_features = SchemaChecks(dataframe=df_with_features, problem=problem)\n",
    "# # schema_checker_features.check()\n",
    "\n",
    "# # # Re-enable ANSI mode\n",
    "# # spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n",
    "\n",
    "# # Apply Spark ML preprocessing\n",
    "# pre_process_variables = PreProcessVariables(\n",
    "#     dataframe=df_with_features,\n",
    "#     problem=problem,\n",
    "#     schema_checks=schema_checker_features\n",
    "# )\n",
    "\n",
    "# transformed_df, feature_names, feature_output_col, feature_map = pre_process_variables.process()\n",
    "\n",
    "# print(f\"\\nüìä PREPROCESSING SUMMARY:\")\n",
    "# print(f\"  Encoded categorical features: {len(feature_names)}\")\n",
    "# print(f\"  Feature vector column: {feature_output_col}\")\n",
    "# print(f\"  Total columns after preprocessing: {len(transformed_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3fa47d5-614f-4724-9b59-ab29ee980585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 7. üîß Auto Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9ba12d-142d-4b14-a99c-7d39ba1937cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.features.auto_feature_generator import AutoFeatureGenerator\n",
    "\n",
    "# Initialize feature generator\n",
    "feature_gen = AutoFeatureGenerator(\n",
    "    schema_checks=schema_checker,\n",
    "    problem=problem\n",
    ")\n",
    "\n",
    "# Get column lists and exclude target to avoid removal error\n",
    "numerical_cols = schema_checker.get_typed_col(col_type=\"numerical\")\n",
    "categorical_cols = schema_checker.get_typed_col(col_type=\"categorical\")\n",
    "datetime_cols = schema_checker.get_typed_col(col_type=\"datetime\")\n",
    "\n",
    "# Remove target from all lists if present\n",
    "target = problem.target\n",
    "if target in numerical_cols:\n",
    "    numerical_cols.remove(target)\n",
    "if target in categorical_cols:\n",
    "    categorical_cols.remove(target)\n",
    "if target in datetime_cols:\n",
    "    datetime_cols.remove(target)\n",
    "\n",
    "# Generate all features with explicit column lists\n",
    "df_with_features = feature_gen.generate_all_features(\n",
    "    include_numerical=True,\n",
    "    include_interactions=True,\n",
    "    include_binning=True,\n",
    "    include_datetime=True,\n",
    "    include_string=False,\n",
    "    numerical_columns=numerical_cols,\n",
    "    categorical_columns=categorical_cols,\n",
    "    datetime_columns=datetime_cols\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä FEATURE GENERATION SUMMARY:\")\n",
    "print(f\"  Original features: {len(df.columns)}\")\n",
    "print(f\"  Total features: {len(df_with_features.columns)}\")\n",
    "print(f\"  New features generated: {len(df_with_features.columns) - len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7384e0-0a09-411a-9ce6-9efd1d87f8d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 8. üîÑ Feature Preprocessing (Spark ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f436a388-26c1-4aed-80e5-76741af0aa39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.features.process import PreProcessVariables\n",
    "from pyspark.sql import functions as F\n",
    "from backend.core.discovery import SchemaChecks\n",
    "\n",
    "# IMPORTANT: Restore original method first to avoid recursion from previous runs\n",
    "schema_checker.get_typed_col = SchemaChecks.get_typed_col.__get__(schema_checker, SchemaChecks)\n",
    "\n",
    "# Get categorical columns and filter by cardinality to avoid model size overflow\n",
    "categorical_cols = schema_checker.get_typed_col(col_type=\"categorical\")\n",
    "\n",
    "# Remove target if present\n",
    "if problem.target in categorical_cols:\n",
    "    categorical_cols.remove(problem.target)\n",
    "\n",
    "# Filter out high-cardinality columns to keep model size under 268MB limit\n",
    "# Only keep columns with < 20 unique values (reduced from 100 to exclude state column)\n",
    "print(\"Filtering categorical columns by cardinality...\")\n",
    "low_cardinality_cols = []\n",
    "for col in categorical_cols:\n",
    "    distinct_count = df_with_features.select(F.countDistinct(col)).collect()[0][0]\n",
    "    if distinct_count < 20:\n",
    "        low_cardinality_cols.append(col)\n",
    "        print(f\"  ‚úì {col}: {distinct_count} unique values\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {col}: {distinct_count} unique values (excluded)\")\n",
    "\n",
    "print(f\"\\nUsing {len(low_cardinality_cols)} of {len(categorical_cols)} categorical columns\")\n",
    "\n",
    "# Reduce numerical features to avoid model size overflow\n",
    "# Keep only original numerical features + a few key engineered ones\n",
    "print(\"\\nReducing numerical features to fit model size limit...\")\n",
    "all_numerical_cols = schema_checker.get_typed_col(col_type=\"numerical\")\n",
    "if problem.target in all_numerical_cols:\n",
    "    all_numerical_cols.remove(problem.target)\n",
    "\n",
    "# Keep only first 15 numerical features (reduced from 30 to fit under 268MB limit)\n",
    "reduced_numerical_cols = all_numerical_cols[:15]\n",
    "print(f\"  Using {len(reduced_numerical_cols)} of {len(all_numerical_cols)} numerical features\")\n",
    "\n",
    "# Drop unused numerical columns from dataframe\n",
    "cols_to_keep = [problem.target] + low_cardinality_cols + reduced_numerical_cols + [problem.date_column]\n",
    "cols_to_keep = [c for c in cols_to_keep if c in df_with_features.columns]\n",
    "df_reduced = df_with_features.select(*cols_to_keep)\n",
    "\n",
    "print(f\"  Reduced from {len(df_with_features.columns)} to {len(df_reduced.columns)} columns\")\n",
    "\n",
    "# Sample data to reduce model size (fit on 0.25% of data to stay under 268MB limit)\n",
    "print(\"\\nSampling data to reduce model size...\")\n",
    "df_sample = df_reduced.sample(fraction=0.00025, seed=42)\n",
    "sample_count = df_sample.count()\n",
    "print(f\"  Sample size: {sample_count:,} rows ({sample_count/df_reduced.count()*100:.2f}%)\")\n",
    "\n",
    "# Patch schema_checker to return filtered columns\n",
    "original_get_typed_col = SchemaChecks.get_typed_col\n",
    "def patched_get_typed_col(self, col_type):\n",
    "    if col_type == \"categorical\":\n",
    "        return low_cardinality_cols\n",
    "    elif col_type == \"numerical\":\n",
    "        return reduced_numerical_cols\n",
    "    return original_get_typed_col(self, col_type)\n",
    "\n",
    "schema_checker.get_typed_col = patched_get_typed_col.__get__(schema_checker, SchemaChecks)\n",
    "\n",
    "# Apply Spark ML preprocessing (fit on sample)\n",
    "train_dataset = df_sample\n",
    "\n",
    "pre_process_variables = PreProcessVariables(\n",
    "    dataframe=df_with_features,\n",
    "    problem=problem,\n",
    "    schema_checks=schema_checker,\n",
    "    train_dataframe = train_dataset\n",
    ")\n",
    "\n",
    "transformed_df, feature_names, feature_output_col, feature_map = pre_process_variables.process()\n",
    "\n",
    "# Restore original method\n",
    "schema_checker.get_typed_col = original_get_typed_col.__get__(schema_checker, SchemaChecks)\n",
    "\n",
    "print(f\"\\nüìä PREPROCESSING SUMMARY:\")\n",
    "print(f\"  Encoded categorical features: {len(feature_names)}\")\n",
    "print(f\"  Feature vector column: {feature_output_col}\")\n",
    "print(f\"  Total columns after preprocessing: {len(transformed_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda4d096-dc8e-4841-b54d-3d97ec0aae70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0146a337-fb77-4ba1-ac05-cd2568eea017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 9. üìä Baseline Models (NEW)\n",
    "\n",
    "The `BaselineModels` class trains simple models to establish performance baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad04e793-2e87-4e05-9ceb-4fbc678537a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.models.baseline_models import BaselineModels\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = transformed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Train set: {train_df.count():,} rows\")\n",
    "print(f\"Test set: {test_df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26cca519-95c5-4c46-805f-f71c878fd615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize baseline models\n",
    "baselines = BaselineModels(\n",
    "    problem=problem,\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    feature_col=feature_output_col,\n",
    "    label_col=problem.target\n",
    ")\n",
    "\n",
    "print(\"\\nüìä TRAINING BASELINE MODELS...\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "576315bb-3a3a-4379-a9f0-ecc91fdffa2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train naive baseline (always predicts majority class)\n",
    "print(\"\\n1Ô∏è‚É£ Naive Baseline...\")\n",
    "naive_result = baselines.train_naive_baseline()\n",
    "print(f\"   Accuracy: {naive_result.metrics.get('accuracy', 0):.4f}\")\n",
    "print(f\"   Training time: {naive_result.training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd1bc02-6b51-403e-9b66-520fe6ac2c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "print(\"\\n2Ô∏è‚É£ Decision Tree...\")\n",
    "dt_result = baselines.train_decision_tree(max_depth=5)\n",
    "print(f\"   Accuracy: {dt_result.metrics.get('accuracy', 0):.4f}\")\n",
    "print(f\"   F1: {dt_result.metrics.get('f1', 0):.4f}\")\n",
    "print(f\"   Training time: {dt_result.training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac7085e9-0478-4d44-8656-4a86f492341d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"\\n3Ô∏è‚É£ Logistic Regression...\")\n",
    "lr_result = baselines.train_logistic_regression()\n",
    "print(f\"   Accuracy: {lr_result.metrics.get('accuracy', 0):.4f}\")\n",
    "print(f\"   F1: {lr_result.metrics.get('f1', 0):.4f}\")\n",
    "print(f\"   AUC: {lr_result.metrics.get('auc', 0):.4f}\")\n",
    "print(f\"   Training time: {lr_result.training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "930bd369-5aa9-4477-a7e4-52a943db2de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all baseline results\n",
    "import pandas as pd\n",
    "\n",
    "all_results = baselines.get_results()\n",
    "\n",
    "print(\"\\nüìä BASELINE MODELS SUMMARY:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "results_data = []\n",
    "for result in all_results:\n",
    "    results_data.append({\n",
    "        'Model': result.model_name,\n",
    "        'Accuracy': result.metrics.get('accuracy', 0),\n",
    "        'F1': result.metrics.get('f1', 0),\n",
    "        'AUC': result.metrics.get('auc', 0),\n",
    "        'Time (s)': result.training_time\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log all baseline models to MLflow\n",
    "print(\"\\nüì° Logging baseline models to MLflow...\")\n",
    "for result in all_results:\n",
    "    run_id = tracker.log_baseline_run(\n",
    "        model_name=result.model_name,\n",
    "        metrics=result.metrics,\n",
    "        training_time=result.training_time,\n",
    "        params={\"model_name\": result.model_name},\n",
    "    )\n",
    "    status = f\"run_id={run_id}\" if run_id else \"skipped\"\n",
    "    print(f\"  {result.model_name}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710cc3d5-3fed-4691-abfe-b184277086c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 10. üéØ XGBoost Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904cba17-5fbd-497b-96fe-d13bf80ae650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.features.feature_selector import FeatureSelector\n",
    "\n",
    "# Train XGBoost model\n",
    "feature_selector = FeatureSelector(\n",
    "    problem=problem,\n",
    "    transformed_df=transformed_df,\n",
    "    feature_names=feature_names,\n",
    "    feature_col=feature_output_col,\n",
    "    feature_idx_name_mapping=feature_map,\n",
    "    train_split=0.8\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost model...\")\n",
    "feature_selector.train_model()\n",
    "print(\"‚úÖ XGBoost training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42259ca0-9fa8-458d-9cf5-32477f55b66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate XGBoost\n",
    "print(\"\\nüìä XGBOOST EVALUATION:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nTrain Set:\")\n",
    "train_metrics = feature_selector.evaluate(train=True)\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "test_metrics = feature_selector.evaluate(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log XGBoost training run to MLflow\n",
    "print(\"\\nüì° Logging XGBoost run to MLflow...\")\n",
    "\n",
    "xgb_params = {\n",
    "    \"num_round\": 100,\n",
    "    \"max_depth\": 4,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"num_features\": len(feature_names),\n",
    "    \"train_split\": 0.8,\n",
    "}\n",
    "\n",
    "run_id = tracker.log_xgboost_run(\n",
    "    params=xgb_params,\n",
    "    train_metrics=train_metrics,\n",
    "    test_metrics=test_metrics,\n",
    ")\n",
    "print(f\"  XGBoost: run_id={run_id}\" if run_id else \"  XGBoost: skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "becb22c6-4325-4e50-8e5f-132f7efe67c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "print(\"\\nüéØ TOP 20 FEATURES BY IMPORTANCE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "importance_list = feature_selector.get_feature_importances()\n",
    "for i, (feature, importance) in enumerate(importance_list[:20]):\n",
    "    print(f\"  {i+1:2d}. {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e09af4e-7eed-4dae-a2cd-be1c7b83cb28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 10.1 üîç SHAP Analysis (NEW)\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) provides detailed insights into how each feature contributes to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46503093-f12b-4aaf-bcde-e2b4427d1643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SHAP Analysis - Feature importance based on Shapley values\n",
    "print(\"\\nüîç SHAP ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run SHAP analysis (this may take a minute)\n",
    "shap_results = feature_selector.get_shap_analysis(\n",
    "    sample_size=1000,  # Use 1000 samples for faster analysis\n",
    "    plot=True,\n",
    "    plot_type='all'  # Generate both summary and bar plots\n",
    ")\n",
    "\n",
    "print(\"\\nüìä SHAP Feature Importance (Top 15):\")\n",
    "print(\"-\" * 40)\n",
    "print(shap_results['feature_importance'].head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e4f1ac-e0b2-4fa8-9263-4fc47e3b4bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get detailed feature value impacts from tree splits\n",
    "print(\"\\nüìä FEATURE VALUE IMPACTS (Aggregated from Tree Splits):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "feature_impacts = feature_selector.get_feature_value_impacts(top_n=15)\n",
    "print(feature_impacts.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d2779c-3303-409b-831c-96dec3045b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explain a single prediction\n",
    "print(\"\\nüîé EXPLAINING A SINGLE PREDICTION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Explain the first test instance\n",
    "explanation = feature_selector.explain_prediction(instance_idx=0, use_test=True)\n",
    "\n",
    "print(f\"\\nInstance Index: {explanation['instance_idx']}\")\n",
    "print(f\"Prediction: {explanation['prediction']}\")\n",
    "print(f\"Probability: {explanation['probability']}\")\n",
    "print(f\"Actual Label: {explanation['actual_label']}\")\n",
    "print(f\"Base Value: {explanation['base_value']:.4f}\")\n",
    "\n",
    "print(\"\\nüìà Top 5 Positive Contributors (pushing toward positive class):\")\n",
    "print(explanation['top_positive'][['Feature', 'Value', 'SHAP_Value']].to_string(index=False))\n",
    "\n",
    "print(\"\\nüìâ Top 5 Negative Contributors (pushing toward negative class):\")\n",
    "print(explanation['top_negative'][['Feature', 'Value', 'SHAP_Value']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa843001-c153-4528-a79a-32ad50222a6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate waterfall plot for the prediction\n",
    "print(\"\\nüìä Generating SHAP Waterfall Plot...\")\n",
    "feature_selector.plot_shap_waterfall(instance_idx=0, use_test=True)\n",
    "\n",
    "# Display the saved plot\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename='shap_waterfall_instance_0.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd92709-6a3a-461f-91d8-8f89d57fe82d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 10.2 üìä Feature Insight Analysis (NEW)\n",
    "\n",
    "Feature Insight Analysis provides SparkBeyond-style discovery of feature conditions that have high **Lift** (how much better than baseline), **Support** (coverage), and **RIG** (Relative Information Gain).\n",
    "\n",
    "This helps identify:\n",
    "- Which feature values are most predictive of the target\n",
    "- Microsegments (combinations of features) that perform even better\n",
    "- Optimal trade-offs between lift and support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a22ae4-fcfb-4844-9286-8c8a3cd89dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.features.insight_analyzer import FeatureInsightAnalyzer, quick_insight_analysis\n",
    "\n",
    "print(\"üìä FEATURE INSIGHT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAnalyzing features for lift, support, and RIG...\")\n",
    "\n",
    "# Initialize the analyzer\n",
    "insight_analyzer = FeatureInsightAnalyzer(\n",
    "    df=df_with_features,  # Use original data (not transformed)\n",
    "    problem=problem,\n",
    "    schema_checks=schema_checker,\n",
    "    n_bins=10,          # Number of bins for numeric features\n",
    "    min_support=0.01,   # Minimum 1% support\n",
    "    min_lift=1.1        # Minimum 10% lift over baseline\n",
    ")\n",
    "\n",
    "# Run analysis\n",
    "result = insight_analyzer.get_analysis_result(discover_microsegments=True)\n",
    "\n",
    "print(f\"\\nüìà Analysis Summary:\")\n",
    "print(f\"  Target Class: {result.target_class}\")\n",
    "print(f\"  Baseline Rate: {result.baseline_rate*100:.2f}%\")\n",
    "print(f\"  Total Records: {result.total_count:,}\")\n",
    "print(f\"  Total Insights Found: {result.summary['total_insights']}\")\n",
    "print(f\"  Microsegments Found: {result.summary['total_microsegments']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c244c3-f938-452e-a026-e6410c451460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b0f299-c4cc-43db-a2fb-db71f9f696bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display top insights\n",
    "print(\"\\nüéØ TOP 20 FEATURE INSIGHTS (Sorted by Lift):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "insights_df = insight_analyzer.to_dataframe(top_n=20)\n",
    "display_cols = ['Condition', 'Lift', 'Support', 'Support_Count', 'RIG', 'Class_Rate']\n",
    "print(insights_df[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbf8b62-7e37-4872-8927-2f8d225b0a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display microsegments (feature combinations)\n",
    "print(\"\\nüîó TOP MICROSEGMENTS (Feature Combinations):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if result.microsegments:\n",
    "    for i, micro in enumerate(result.microsegments[:10], 1):\n",
    "        print(f\"\\n{i}. {micro.name}\")\n",
    "        print(f\"   Lift: x{micro.lift:.2f} | Support: {micro.support*100:.1f}% ({micro.support_count:,}) | RIG: {micro.rig:.3f}\")\n",
    "        print(f\"   Class Rate: {micro.class_rate*100:.1f}% vs Baseline: {micro.baseline_rate*100:.1f}%\")\n",
    "else:\n",
    "    print(\"No microsegments found that improve over individual features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a4c163-7481-452d-9f58-30147ac66f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate Lift vs Support scatter plot (like SparkBeyond UI)\n",
    "print(\"\\nüìä Generating Lift vs Support Scatter Plot...\")\n",
    "insight_analyzer.plot_lift_support_scatter(\n",
    "    top_n=50,\n",
    "    highlight_microsegments=True,\n",
    "    save_path='insight_lift_support.png'\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename='insight_lift_support.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30383089-56ea-4ea8-83b7-b62063767905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate top insights bar chart\n",
    "print(\"\\nüìä Generating Top Insights Bar Chart...\")\n",
    "insight_analyzer.plot_top_insights(\n",
    "    top_n=15,\n",
    "    metric='lift',\n",
    "    save_path='insight_top_features.png'\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "display(Image(filename='insight_top_features.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90af6d91-cd1a-4d1d-b8d1-ac79e5def2a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You can also use the quick function for rapid analysis\n",
    "# insights_df, result = quick_insight_analysis(df, problem, schema_checker, top_n=20, plot=True)\n",
    "\n",
    "# Or access insights through FeatureSelector after training\n",
    "# insight_result = feature_selector.get_insight_analysis(schema_checker, plot=True)\n",
    "\n",
    "print(\"\\n‚úÖ Feature Insight Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "711ad8c1-e14c-407a-90b4-d34b04739397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 11. ü§ñ LightAutoML (NEW)\n",
    "\n",
    "The `AutoMLRunner` provides automated machine learning using LightAutoML,\n",
    "which automatically tries multiple algorithms and finds the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d43dc0-6ad1-41a3-a771-339f3aed2b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.models.evalml_runner import AutoMLRunner, quick_automl\n",
    "\n",
    "print(\"ü§ñ LIGHTAUTOML SEARCH\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nThis may take a few minutes...\\n\")\n",
    "\n",
    "# Initialize AutoML runner\n",
    "automl_runner = AutoMLRunner(\n",
    "    spark=spark,\n",
    "    problem=problem,\n",
    "    max_rows_for_pandas=50000,  # Sample for faster demo\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run AutoML search: Quick Mode\n",
    "automl_result = automl_runner.run_automl(\n",
    "    spark_df=df_with_features,\n",
    "    timeout=120,  # 2 minutes for demo\n",
    "    cpu_limit=4,\n",
    "    quick_mode=True\n",
    ")\n",
    "\n",
    "# # Run AutoML search: Full Mode\n",
    "# automl_result = automl_runner.run_automl(\n",
    "#     spark_df=df_with_features,\n",
    "#     timeout=600,  # 2 minutes for demo\n",
    "#     cpu_limit=4,\n",
    "# )\n",
    "\n",
    "print(\"\\n‚úÖ AutoML search complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f76d482-85a2-46e4-ae2b-8233fbae3648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display AutoML results\n",
    "print(\"\\nüìä AUTOML RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Best Score: {automl_result.best_score:.4f}\")\n",
    "print(f\"  Problem Type: {automl_result.problem_type}\")\n",
    "print(f\"  Metric: {automl_result.metric}\")\n",
    "print(f\"  Search Time: {automl_result.search_time:.1f}s\")\n",
    "\n",
    "if automl_result.model_summary:\n",
    "    print(f\"\\n  Models in ensemble: {automl_result.model_summary.get('n_models', 0)}\")\n",
    "    print(f\"  Levels: {automl_result.model_summary.get('levels', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ecd7b8-ac36-454a-abfa-078d6d8fa37c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AutoML feature importance\n",
    "if automl_result.feature_importance is not None:\n",
    "    print(\"\\nüéØ AUTOML TOP FEATURES:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(automl_result.feature_importance.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7861d72-d759-4855-881d-ae606590d9b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate AutoML on test data\n",
    "print(\"\\nüìä AUTOML EVALUATION ON TEST DATA:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "automl_metrics = automl_runner.evaluate(df_with_features)\n",
    "for metric, value in automl_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log AutoML run to MLflow\n",
    "print(\"\\nüì° Logging AutoML run to MLflow...\")\n",
    "\n",
    "run_id = tracker.log_automl_run(\n",
    "    config={\n",
    "        \"timeout\": 120,\n",
    "        \"cpu_limit\": 4,\n",
    "        \"quick_mode\": True,\n",
    "        \"max_rows_for_pandas\": 50000,\n",
    "    },\n",
    "    best_score=automl_result.best_score,\n",
    "    search_time=automl_result.search_time,\n",
    "    problem_type=automl_result.problem_type,\n",
    "    metric=automl_result.metric,\n",
    ")\n",
    "print(f\"  AutoML: run_id={run_id}\" if run_id else \"  AutoML: skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8642400a-8db2-446f-ae60-d5819e91cecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 12. üî¨ Model Comparison (NEW)\n",
    "\n",
    "The `ModelComparison` framework tracks experiments and measures the impact\n",
    "of feature engineering and model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8612284-afe9-4287-8ddf-3d0275334cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from backend.core.models.model_comparison import ModelComparison\n",
    "\n",
    "# Initialize comparison framework\n",
    "comparison = ModelComparison(primary_metric='accuracy')\n",
    "\n",
    "# Add baseline results\n",
    "for result in all_results:\n",
    "    comparison.add_baseline_result(result)\n",
    "\n",
    "# Add XGBoost result\n",
    "comparison.add_experiment(\n",
    "    name=\"XGBoost + Engineered Features\",\n",
    "    model_name=\"XGBoost\",\n",
    "    feature_set=\"engineered\",\n",
    "    metrics=test_metrics,\n",
    "    training_time=0\n",
    ")\n",
    "\n",
    "# Add AutoML result\n",
    "comparison.add_experiment(\n",
    "    name=\"LightAutoML\",\n",
    "    model_name=\"LightAutoML Ensemble\",\n",
    "    feature_set=\"engineered\",\n",
    "    metrics=automl_metrics,\n",
    "    training_time=automl_result.search_time\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Added all experiments to comparison framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log feature comparison runs to MLflow (XGBoost + AutoML with engineered features)\n",
    "print(\"\\nüì° Logging feature comparison runs to MLflow...\")\n",
    "\n",
    "run_id = tracker.log_feature_comparison_run(\n",
    "    model_name=\"XGBoost\",\n",
    "    feature_set=\"engineered\",\n",
    "    metrics=test_metrics,\n",
    "    training_time=0,\n",
    "    num_features=len(feature_names),\n",
    ")\n",
    "print(f\"  XGBoost (engineered): run_id={run_id}\" if run_id else \"  XGBoost (engineered): skipped\")\n",
    "\n",
    "run_id = tracker.log_feature_comparison_run(\n",
    "    model_name=\"LightAutoML Ensemble\",\n",
    "    feature_set=\"engineered\",\n",
    "    metrics=automl_metrics,\n",
    "    training_time=automl_result.search_time,\n",
    "    num_features=len(df_with_features.columns),\n",
    ")\n",
    "print(f\"  LightAutoML (engineered): run_id={run_id}\" if run_id else \"  LightAutoML (engineered): skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a397b365-f9fd-47d2-8e11-1701d5165cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get comparison results\n",
    "comparison_result = comparison.get_comparison()\n",
    "\n",
    "print(\"\\nüî¨ MODEL COMPARISON:\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_result.comparison_table.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b49eeb-c071-44ea-9169-58c4a61f5a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display improvements\n",
    "print(\"\\nüìà IMPACT ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if comparison_result.improvements:\n",
    "    for impact_name, impact_data in comparison_result.improvements.items():\n",
    "        print(f\"\\n{impact_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Absolute Improvement: {impact_data.get('absolute_improvement', 0):.4f}\")\n",
    "        print(f\"  Percentage Improvement: {impact_data.get('percentage_improvement', 0):.2f}%\")\n",
    "        print(f\"  Best Model: {impact_data.get('best_model', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f652612-6f42-4680-bd0c-242b7601a26c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best model\n",
    "print(\"\\nüèÜ BEST MODEL:\")\n",
    "print(\"-\" * 50)\n",
    "best = comparison_result.best_experiment\n",
    "print(f\"  Name: {best.name}\")\n",
    "print(f\"  Model: {best.model_name}\")\n",
    "print(f\"  Feature Set: {best.feature_set}\")\n",
    "print(f\"  {comparison.primary_metric.title()}: {best.metrics.get(comparison.primary_metric, 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. MLflow Experiment Summary\n",
    "\n",
    "View all tracked runs from this session. These runs are stored locally in `./mlruns/` and can also be viewed with the MLflow UI by running `mlflow ui` from the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and display all MLflow runs for this experiment\n",
    "print(\"MLflow EXPERIMENT RUNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "runs = tracker.get_runs(max_results=50)\n",
    "\n",
    "if runs:\n",
    "    import pandas as pd\n",
    "\n",
    "    runs_data = []\n",
    "    for run in runs:\n",
    "        row = {\n",
    "            \"Run Name\": run[\"run_name\"],\n",
    "            \"Status\": run[\"status\"],\n",
    "            \"Model Type\": run[\"tags\"].get(\"model_type\", \"\"),\n",
    "        }\n",
    "        # Add key metrics\n",
    "        for metric_name in [\"accuracy\", \"f1\", \"auc\", \"best_score\",\n",
    "                            \"test_accuracy\", \"test_f1\", \"test_auc\",\n",
    "                            \"training_time_seconds\"]:\n",
    "            if metric_name in run[\"metrics\"]:\n",
    "                row[metric_name] = round(run[\"metrics\"][metric_name], 4)\n",
    "        runs_data.append(row)\n",
    "\n",
    "    runs_df = pd.DataFrame(runs_data)\n",
    "    print(f\"\\nTotal runs: {len(runs)}\")\n",
    "    print(f\"Experiment: {tracker.get_experiment_info()['name']}\\n\")\n",
    "    display(runs_df)\n",
    "else:\n",
    "    print(\"No MLflow runs found for this experiment.\")\n",
    "\n",
    "print(f\"\\nTo launch the MLflow UI, run: mlflow ui --backend-store-uri file://<path-to-mlruns>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21793708-4d27-4a7a-b00c-0e55ad3136c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated all the features in Spark Tune:\n",
    "\n",
    "| Feature | Module | Description |\n",
    "|---------|--------|-------------|\n",
    "| **MLflow Tracking** | `services.mlflow_service` | Experiment tracking for all model runs |\n",
    "| Data Quality Checking | `core.profiling.data_quality` | PySpark-native quality analysis |\n",
    "| YData Profiling | `core.profiling.ydata_profiler` | Comprehensive data profiling |\n",
    "| Time Series Detection | `core.utils.time_series_detector` | Automatic temporal pattern detection |\n",
    "| Enhanced Preprocessing | `core.features.preprocessing_enhanced` | Missing values, outliers, scaling |\n",
    "| Baseline Models | `core.models.baseline_models` | Simple model comparison |\n",
    "| **SHAP Analysis** | `core.features.feature_selector` | Model interpretability with SHAP |\n",
    "| **Feature Insights** | `core.features.insight_analyzer` | Lift, Support, RIG analysis (SparkBeyond-style) |\n",
    "| LightAutoML | `core.models.evalml_runner` | Automated machine learning |\n",
    "| Model Comparison | `core.models.model_comparison` | Impact analysis framework |\n",
    "\n",
    "### Key Metrics in Feature Insight Analysis:\n",
    "- **Lift**: How much better a feature condition performs vs. baseline (e.g., x3.09 = 3x better)\n",
    "- **Support**: Percentage of data covered by the condition (e.g., 25% = 10,234 records)\n",
    "- **RIG**: Relative Information Gain - how much information the feature provides about the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3987808a-e775-4d7a-b187-e5d64ef90e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "# spark.stop()\n",
    "# print(\"\\n‚úÖ Spark session stopped. Demo complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a189c24-9a5c-4f68-8b8b-272a5dadce75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/yadvendra@aidetic.in/spark_beyond/requirements.txt"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "enhanced_features_demo_ hdfc",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "spark-beyond",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
